{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#read in the files\n",
    "file = open(\"in.dta.txt\", \"r\")\n",
    "lines = file.readlines()\n",
    "x_points = []\n",
    "outputs = []\n",
    "#separate outputs from points\n",
    "for x in lines:\n",
    "    x_points.append([1 , float(x.split('  ')[1]), float(x.split('  ')[2])])\n",
    "    outputs.append(float(x.split('  ')[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = x_points[0:25]\n",
    "train_out = outputs[0:25]\n",
    "validation = x_points[25:]\n",
    "validate_out = outputs[25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the points\n",
    "def transformation(point, k):\n",
    "    x1 = point[1]\n",
    "    x2 = point[2]\n",
    "    phi = [1, x1, x2, np.square(x1), np.square(x2), x1*x2, np.absolute(x1 - x2), np.absolute(x1 + x2)]\n",
    "    phi_k = []\n",
    "    for i in range(k+1):\n",
    "        phi_k.append(phi[i])\n",
    "    return phi_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression function with datasets as parameters\n",
    "def lin_dataset(data, output, x_test, y_test):\n",
    "    #compute pseudo-inverse\n",
    "    xTX = np.dot(np.transpose(data), data)\n",
    "    pseudo = np.dot(np.linalg.inv(xTX), np.transpose(data))\n",
    "    weights = np.dot(pseudo, output)\n",
    "\n",
    "    incorrect = 0\n",
    "    correct = 0\n",
    "    for j in range(len(data)):\n",
    "        #find classification using line\n",
    "        f_output = output[j]\n",
    "        #find classification using weights\n",
    "        g_output = np.sign(np.dot(weights, data[j]))\n",
    "        #check accuracy\n",
    "        if f_output != g_output:\n",
    "            incorrect +=1\n",
    "        else:\n",
    "            correct += 1\n",
    "    #calculate probability of accuracy\n",
    "    E_in = incorrect/(incorrect + correct)\n",
    "    \n",
    "    incorrect_test = 0\n",
    "    correct_test = 0\n",
    "    for g in range(len(x_test)):\n",
    "        #find classification using line\n",
    "        f_out = y_test[g]\n",
    "        #find classification using weights\n",
    "        g_out = np.sign(np.dot(weights, x_test[g]))\n",
    "        #check accuracy\n",
    "        if f_out != g_out:\n",
    "            incorrect_test +=1\n",
    "        else:\n",
    "            correct_test += 1\n",
    "    #calculate probability of accuracy\n",
    "    E_out = incorrect_test/(incorrect_test + correct_test)\n",
    "    \n",
    "    return E_in, E_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 3 : 0.3\n",
      "k = 4 : 0.5\n",
      "k = 5 : 0.2\n",
      "k = 6 : 0.0\n",
      "k = 7 : 0.1\n"
     ]
    }
   ],
   "source": [
    "#loop through different values of k (for transformation variation)\n",
    "for i in range(3, 8):\n",
    "    transform_data = []\n",
    "    transform_validate = []\n",
    "    for item in training:\n",
    "        transform = transformation(item, i)\n",
    "        transform_data.append(transform) \n",
    "    for value in validation:\n",
    "        transform_t = transformation(value, i)\n",
    "        transform_validate.append(transform_t)\n",
    "    E_in, E_out = lin_dataset(transform_data, train_out, transform_validate, validate_out)\n",
    "    print(\"k =\", i, \":\", E_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = open(\"out.dta.txt\", \"r\")\n",
    "line = test_set.readlines()\n",
    "test_x = []\n",
    "test_out = []\n",
    "for y in line:\n",
    "    test_x.append([1 , float(y.split('  ')[1]), float(y.split('  ')[2])])\n",
    "    test_out.append(float(y.split('  ')[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 2 : 0.648\n",
      "k = 3 : 0.42\n",
      "k = 4 : 0.416\n",
      "k = 5 : 0.188\n",
      "k = 6 : 0.084\n",
      "k = 7 : 0.072\n"
     ]
    }
   ],
   "source": [
    "#loop through different values of k (for transformation variation)\n",
    "for i in range(2, 8):\n",
    "    transform_test = []\n",
    "    transform_data = []\n",
    "    for item in training:\n",
    "        transform = transformation(item, i)\n",
    "        transform_data.append(transform) \n",
    "    for value in test_x:\n",
    "        transform_t = transformation(value, i)\n",
    "        transform_test.append(transform_t)\n",
    "    E_in, E_out = lin_dataset(transform_data, train_out, transform_test, test_out)\n",
    "    print(\"k =\", i, \":\", E_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 3 : 0.28\n",
      "k = 4 : 0.36\n",
      "k = 5 : 0.2\n",
      "k = 6 : 0.08\n",
      "k = 7 : 0.12\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 8):\n",
    "    transform_data = []\n",
    "    transform_validate = []\n",
    "    for item in training:\n",
    "        transform = transformation(item, i)\n",
    "        transform_data.append(transform) \n",
    "    for value in validation:\n",
    "        transform_t = transformation(value, i)\n",
    "        transform_validate.append(transform_t)\n",
    "    E_in, E_out = lin_dataset(transform_validate, validate_out, transform_data, train_out)    \n",
    "    print(\"k =\", i, \":\", E_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 3 : 0.396\n",
      "k = 4 : 0.388\n",
      "k = 5 : 0.284\n",
      "k = 6 : 0.192\n",
      "k = 7 : 0.196\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 8):\n",
    "    transform_test = []\n",
    "    transform_validate = []\n",
    "    for item in validation:\n",
    "        transform = transformation(item, i)\n",
    "        transform_validate.append(transform) \n",
    "    for value in test_x:\n",
    "        transform_t = transformation(value, i)\n",
    "        transform_test.append(transform_t)\n",
    "    E_in, E_out = lin_dataset(transform_validate, validate_out, transform_test, test_out)\n",
    "    print(\"k =\", i, \":\", E_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6: This is closest to 0.5, 0.5, 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33333976337718557\n"
     ]
    }
   ],
   "source": [
    "all_e = []\n",
    "for a in range(100000):\n",
    "    e1 = np.random.uniform(0, 1)\n",
    "    e2 = np.random.uniform(0, 1)\n",
    "    e = min(e1, e2)\n",
    "    all_e.append(e)\n",
    "print(np.average(all_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.sqrt(np.sqrt(3) + 4)\n",
    "b = np.sqrt(np.sqrt(3) - 1)\n",
    "c = np.sqrt((np.sqrt(6) * 4) + 9)\n",
    "d = np.sqrt((-1* np.sqrt(6)) + 9)\n",
    "rho_vals = [a, b, c, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_regression_ax_b(xN, outputs):\n",
    "    #compute pseudo-inverse\n",
    "    xTX = np.dot(np.transpose(xN), xN)\n",
    "    pseudo = np.dot(np.linalg.inv(xTX), np.transpose(xN))\n",
    "    weights = np.dot(pseudo, outputs)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho =  2.3941701709713277 constant: [0.25, 1.0, 0.25] average: 0.5\n",
      "rho =  2.3941701709713277 ax+b: [2.057919670001511, 1.0, 0.3472104330563116] average: 1.135043367685941\n",
      "rho =  0.8555996771673521 constant: [0.25, 1.0, 0.25] average: 0.5\n",
      "rho =  0.8555996771673521 ax+b: [191.83315211069248, 1.0, 1.1616931131685837] average: 64.66494840795369\n",
      "rho =  4.335661307243996 constant: [0.25, 1.0, 0.25] average: 0.5\n",
      "rho =  4.335661307243996 ax+b: [0.3594976839169588, 1.0, 0.14050231608304176] average: 0.5000000000000001\n",
      "rho =  2.5593964634688433 constant: [0.25, 1.0, 0.25] average: 0.5\n",
      "rho =  2.5593964634688433 ax+b: [1.6449280309816467, 1.0, 0.3157237570099987] average: 0.9868839293305486\n"
     ]
    }
   ],
   "source": [
    "for r in rho_vals:\n",
    "    rho = r\n",
    "    \n",
    "    #constant model\n",
    "    squared_error_const = []\n",
    "    points = [[1, -1], [1, rho], [1, 1]]\n",
    "    outputs = [0, 1, 0]\n",
    "    for i in range(3):\n",
    "        leave = points.copy()\n",
    "        out = outputs.copy()\n",
    "        #leave one out\n",
    "        del leave[i]\n",
    "        del out[i]\n",
    "        constant = np.average(out)\n",
    "        error = np.square(constant - outputs[i])\n",
    "        squared_error_const.append(error)\n",
    "        \n",
    "    #ax+b model\n",
    "    slope = 0\n",
    "    intercept = 0\n",
    "    squared_error = []\n",
    "    points = [[1, -1], [1, rho], [1, 1]]\n",
    "    outputs = [0, 1, 0]\n",
    "    for i in range(3):\n",
    "        leave = points.copy()\n",
    "        out = outputs.copy()\n",
    "        #leave one out\n",
    "        del leave[i]\n",
    "        del out[i]\n",
    "        weight = lin_regression_ax_b(leave, out)\n",
    "        slope = (weight[1])\n",
    "        intercept = (weight[0])\n",
    "        error = np.square(((slope*points[i][1]) + intercept) - outputs[i])\n",
    "        squared_error.append(error)\n",
    "        \n",
    "    print(\"rho = \", rho, \"constant:\", squared_error_const, \"average:\", np.average(squared_error_const))\n",
    "    print(\"rho = \", rho, \"ax+b:\", squared_error, \"average:\", np.average(squared_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pla_svm(N):\n",
    "    #random coordinates chosen to use as our line\n",
    "    coordinate_one = np.random.uniform(-1, 1, 2)\n",
    "    coordinate_two = np.random.uniform(-1, 1, 2)\n",
    "\n",
    "    xN = []\n",
    "    for i in range(N): \n",
    "        #defining the first component as x0 which is always 1\n",
    "        coordinate = [1]\n",
    "        #create dataset of N random points\n",
    "        coordinate[1:2] = np.random.uniform(-1, 1, 2)\n",
    "        xN.append(coordinate)\n",
    "\n",
    "    def det_output(points, coordinate_one, coordinate_two):\n",
    "        #using the determinant to figure out whether the points lie on one side of the line or the other\n",
    "        #formula: (x-x1)(y2-y1) - (y-y1)(x2-x1) where the point is (x, y)\n",
    "        AD = (points[1] - coordinate_one[0]) * (coordinate_two[1] - coordinate_one[1])\n",
    "        BC = (points[2] - coordinate_one[1]) * (coordinate_two[0] - coordinate_one[0])\n",
    "        return np.sign(AD-BC)\n",
    "\n",
    "    outputs = []\n",
    "    for x in xN:\n",
    "        #create a list of outputs using det_output function for each point in dataset\n",
    "        sign = det_output(x, coordinate_one, coordinate_two)\n",
    "        outputs.append(int(sign))\n",
    "        \n",
    "    if(sum(outputs)==N or sum(outputs)==(-1*N)):\n",
    "        return 2, 0\n",
    "\n",
    "    #initial weight vector with all zeroed components\n",
    "    w = [0, 0, 0]\n",
    "\n",
    "    iteration = 0\n",
    "    #all points are misclassified to begin with. create a list with indices of all the points\n",
    "    misclassified = list(range(N))\n",
    "    while len(misclassified) > 0:\n",
    "        #pick a random misclassified index\n",
    "        index = random.choice(misclassified)\n",
    "        #update weight vector\n",
    "        w = w + np.dot(outputs[index], xN[index])\n",
    "        iteration += 1\n",
    "        #if there are no misclassified points remaining, exit while loop\n",
    "        if misclassified ==[]:\n",
    "            update == True\n",
    "        #re-update set of misclassified points by checking whether the weight vector results \n",
    "        #in the same output\n",
    "        misclassified = []\n",
    "        for i in range(N):\n",
    "            if np.sign(np.dot(w, xN[i])) != outputs[i]:\n",
    "                misclassified.append(i)\n",
    "    \n",
    "    #testing with random points\n",
    "    points_data = []\n",
    "    for i in range(1000):\n",
    "        point = [1]\n",
    "        #generate points\n",
    "        point[1:2] = np.random.uniform(-1, 1, 2)\n",
    "        points_data.append(point)\n",
    "    incorrect = 0\n",
    "    correct = 0\n",
    "    for j in range(1000):\n",
    "        #find classification using line\n",
    "        f_output = det_output(points_data[j], coordinate_one, coordinate_two)\n",
    "        #find classification using weights\n",
    "        g_output = np.sign(np.dot(w, points_data[j]))\n",
    "        #check accuracy\n",
    "        if f_output != g_output:\n",
    "            incorrect +=1\n",
    "        else:\n",
    "            correct += 1\n",
    "    #calculate probability of accuracy\n",
    "    probability = incorrect/(incorrect + correct)\n",
    "    \n",
    "    #SVM\n",
    "    model = SVC(C=float(\"inf\"), kernel='linear')\n",
    "    model.fit(xN, outputs)\n",
    "    vectors_num = 0\n",
    "    vectors_num = len(model.support_)\n",
    "    svm_predict = model.predict(points_data)\n",
    "    \n",
    "    corr = 0\n",
    "    incorr = 0\n",
    "    for j in range(1000):\n",
    "        #find classification using line\n",
    "        f_output = det_output(points_data[j], coordinate_one, coordinate_two)\n",
    "        #find classification using weights\n",
    "        g_output = svm_predict[j]\n",
    "        #check accuracy\n",
    "        if f_output != g_output:\n",
    "            incorr +=1\n",
    "        else:\n",
    "            corr += 1\n",
    "    #calculate probability of accuracy\n",
    "    probability_svm = incorr/(incorr + corr)\n",
    "    \n",
    "    if probability > probability_svm:\n",
    "        svm_better = 1\n",
    "    else: \n",
    "        svm_better = 0\n",
    "            \n",
    "    return svm_better, vectors_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5981941309255079\n"
     ]
    }
   ],
   "source": [
    "which_is_better = []\n",
    "for m in range(1000):\n",
    "    better = pla_svm(10)[0]\n",
    "    which_is_better.append(better)\n",
    "filtered = [x for x in which_is_better if x != 2]\n",
    "print(np.average(filtered)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.222\n"
     ]
    }
   ],
   "source": [
    "print(sum(filter(None, which_is_better))/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5813253012048193\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "which_is_better_100 = []\n",
    "support_vecs = []\n",
    "for m in range(1000):\n",
    "    better, supports = pla_svm(100)\n",
    "    which_is_better_100.append(better)\n",
    "    support_vecs.append(supports)\n",
    "filtered = [x for x in which_is_better_100 if x != 2]\n",
    "filtered_vecs = [x for x in support_vecs if x != 0]\n",
    "print(np.average(filtered)) \n",
    "print(np.average(filtered_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6283924843423799\n",
      "2.954070981210856\n"
     ]
    }
   ],
   "source": [
    "which_is_better_20 = []\n",
    "support_vecs = []\n",
    "for m in range(1000):\n",
    "    better, supports = pla_svm(20)\n",
    "    which_is_better_20.append(better)\n",
    "    support_vecs.append(supports)\n",
    "filtered = [x for x in which_is_better_20 if x != 2]\n",
    "filtered_vecs = [x for x in support_vecs if x != 0]\n",
    "print(np.average(filtered)) \n",
    "print(np.average(filtered_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import math\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
